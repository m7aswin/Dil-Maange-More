{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1e20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c229114",
   "metadata": {},
   "source": [
    "### Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fab71a",
   "metadata": {},
   "source": [
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "There are two ways to create RDDs: \n",
    "1. parallelizing an existing collection in your driver program\n",
    "2. referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7523b7",
   "metadata": {},
   "source": [
    "#### Parallelized Collections\n",
    "\n",
    "- Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program.\n",
    "-  The elements of the collection are copied to form a distributed dataset that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fce2285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SparkContext object\n",
    "\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381ec4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[1,2,3,4,5]\n",
    "distData=sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a052be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258aa348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e50aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5250b9f",
   "metadata": {},
   "source": [
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a14c2b5",
   "metadata": {},
   "source": [
    "#### External Datasets\n",
    "\n",
    "- PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "- Text file RDDs can be created using SparkContext’s textFile method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39e7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.txt\n",
    "first \n",
    "second line\n",
    "the third line\n",
    "then a fourth line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cacedf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first ', 'second line', 'the third line', 'then a fourth line']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('example.txt').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ed726",
   "metadata": {},
   "source": [
    "- All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset \n",
    "-  The transformations are only computed when an action requires a result to be returned to the driver program.This design enables Spark to run more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fac28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(1,4)).map(lambda x:(x, 'a'*x))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708061c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first ', 'second line', 'the third line', 'then a fourth line']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=sc.textFile('example.txt')\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37bdf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 11, 14, 18]\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "lineLenghts=lines.map(lambda s:len(s))\n",
    "print(lineLenghts.collect())\n",
    "totalLength=lineLenghts.reduce(lambda a,b:a+b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ff42f",
   "metadata": {},
   "source": [
    "### Creating RDD from Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f14b5",
   "metadata": {},
   "source": [
    "#### Parallelized Collections\n",
    "- RDDs are created by parallelizing an existing collection in your driver program or referencing a dataset in an external storage system.\n",
    "- RDDs are created by taking the existing collection and passing it to SparkContext parallelize() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec8cf397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Simplilearn', 'is']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=['Simplilearn', 'is', 'an', 'educational','revolution']\n",
    "\n",
    "rdd1=sc.parallelize(data)\n",
    "\n",
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1347c",
   "metadata": {},
   "source": [
    "#### Existing Data\n",
    "- RDDs can be created from existing RDDs by transforming one RDD into another RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11c3579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums=rdd1.map(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dae56cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 2, 2, 11, 10]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a473a4",
   "metadata": {},
   "source": [
    "#### External Data\n",
    "- In Spark, a dataset can be created from any other dataset. The other dataset must be supported by Hadoop, including the local file system, HDFS, Cassandra, HBase, and many more.\n",
    "- Data frame reader interface can be used to load dataset from an external storage system in the following formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17249a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c88cf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(company='google', job='sales executive', degree='bachelors', salary_more_then_100k=0),\n",
       " Row(company='google', job='sales executive', degree='masters', salary_more_then_100k=0),\n",
       " Row(company='google', job='business manager', degree='bachelors', salary_more_then_100k=1),\n",
       " Row(company='google', job='business manager', degree='masters', salary_more_then_100k=1),\n",
       " Row(company='google', job='computer programmer', degree='bachelors', salary_more_then_100k=0)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salaries=spark.read.csv('salaries.csv',inferSchema=True,header=True).rdd\n",
    "salaries.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def78ee",
   "metadata": {},
   "source": [
    "#### Creating RDD from a Text File\n",
    "- To create a file-based RDD, you can use the command SparkContext.textFile or sc.textfile, and pass one or more file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d80b4286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first ', 'second line', 'the third line', 'then a fourth line']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=sc.textFile('example.txt')\n",
    "txt.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41af5d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa77f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
